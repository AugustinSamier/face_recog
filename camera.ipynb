{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d150a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "mouth_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_mouth.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3324bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path,create_test=None):\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    file=os.listdir(path)\n",
    "\n",
    "    for entity in file:\n",
    "        try:\n",
    "            entityPath=os.listdir(f\"{path}/{entity}\")\n",
    "            for i,data in enumerate(entityPath):\n",
    "                try:\n",
    "                    current_img=cv2.imread(f\"{path}/{entity}/{data}\")\n",
    "                    if current_img is not None:\n",
    "                        if (create_test is not None and ((len(entityPath)>1) and (i==0))):\n",
    "                            X_test.append(current_img)\n",
    "                            y_test.append(entity)\n",
    "                        else:\n",
    "                            X_train.append(current_img)\n",
    "                            y_train.append(entity)\n",
    "                    else:\n",
    "                        print(f\"{path}/{entity}/{data} is None type\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur avec {path}/{entity}/{data} : {e}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Erreur avec {path}/{entity} : {ex}\")\n",
    "    \n",
    "    print(f\"Number of training sample : {len(X_train)}\\n\")\n",
    "    if create_test is not None:\n",
    "        print(f\"Number of test sample : {len(X_test)}\\n\")\n",
    "        return X_train,y_train,X_test,y_test\n",
    "    else:\n",
    "        return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61009eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_and_eyes(img):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        img=cv2.imread(img)\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60,60)\n",
    "    )\n",
    "\n",
    "    if len(faces)==0:\n",
    "        return [],[]\n",
    "    else:\n",
    "        trueFaces=[]\n",
    "        trueEyes=[]\n",
    "        for i,face in enumerate(faces):\n",
    "            x,y,w,h=face\n",
    "            imgFace=img[y:y+h,x:x+w]\n",
    "            eyes=eye_cascade.detectMultiScale(imgFace)\n",
    "            if len(eyes)>=2:\n",
    "                trueFaces.append(face)\n",
    "                if eyes[0][0]<eyes[1][0]:\n",
    "                    x2=eyes[0][0]\n",
    "                    w2=eyes[1][0]+eyes[1][2]-x2\n",
    "                else:\n",
    "                    x2=eyes[1][0]\n",
    "                    w2=eyes[0][0]+eyes[0][2]-x2\n",
    "                if eyes[0][1]<eyes[1][1]:\n",
    "                    y2=eyes[0][1]\n",
    "                    h2=eyes[1][1]+eyes[1][3]-y2\n",
    "                else:\n",
    "                    y2=eyes[1][1]\n",
    "                    h2=eyes[0][1]+eyes[0][3]-y2\n",
    "                trueEyes.append((x2+x,y2+y,w2,h2))\n",
    "        return trueFaces,trueEyes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c79c6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(img):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        img=cv2.imread(img) #si on passe un path ça read l'img\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60,60)\n",
    "    )\n",
    "\n",
    "    if len(faces)==0:\n",
    "        return img #retourne l'img si aucun visage n'est detecté\n",
    "    \n",
    "    #si je veux detecter tous les visages présents je dois faire une boucle for x,y,w,h in faces: et return une liste d'img crops\n",
    "    x,y,w,h=faces[0] #faces retourne n tuples (x,y,h,w) correspondants aux coords du visage\n",
    "    face=img[y:y+h,x:x+w] #retourne l'img crop aux coords detectées\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ddfba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_face_test=transforms.Compose([\n",
    "    transforms.ToPILImage(), #transforme en format PIL\n",
    "    #transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor() #reconvertit l'img en format tensor\n",
    "])\n",
    "\n",
    "transform_face=transforms.Compose([\n",
    "    transforms.ToPILImage(), #transforme en format PIL\n",
    "    #transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.RandomHorizontalFlip(), #0.5 de proba d'inverser la gauche et la droite de l'img pour rendre le modèle invariant à la symétrie\n",
    "    transforms.RandomRotation(15), #applique rota random entre -10° et +10° \n",
    "    transforms.ColorJitter(brightness=0.3,contrast=0.3,saturation=0.2,hue=0.02), #altère aléatoirement la luminosité et le contraste de l'image\n",
    "    transforms.ToTensor() #reconvertit l'img en format tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9a52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEyesDataset(Dataset):\n",
    "    def __init__(self, X, y, labels, transform_face):\n",
    "        self.X = X  # Liste des chemins ou images\n",
    "        self.y = [labels[label] for label in y]  # Labels convertis en indices\n",
    "        self.transform_face = transform_face\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            img = cv2.imread(img)\n",
    "\n",
    "        face=extract_face(img)\n",
    "\n",
    "        if not isinstance(face, np.ndarray) or face.size == 0:\n",
    "            face = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "\n",
    "        face = self.transform_face(face)\n",
    "\n",
    "        label = self.y[idx]\n",
    "        return face, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef9b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetTrain(X_train,y_train,nb_epoch,labels,batch_size,patience=5,val_split=False):\n",
    "    model=models.resnet50(pretrained=True)\n",
    "    nb_classes=105\n",
    "    model.fc=nn.Linear(model.fc.in_features,nb_classes)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name or \"fc\" in name or \"layer3\" in name:  \n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.fc.parameters(),lr=1e-4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    if val_split:\n",
    "        X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2,random_state=1,stratify=y_train)\n",
    "        val_dataset=FaceEyesDataset(X_val,y_val,labels,transform_face=transform_face_test)\n",
    "        val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "        best_val_acc=0\n",
    "        wait=0\n",
    "\n",
    "    train_dataset=FaceEyesDataset(X_train,y_train,labels,transform_face=transform_face)\n",
    "    train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    tabTrain=[]\n",
    "    tabVal=[]\n",
    "    for epoch in range(nb_epoch):\n",
    "        model.train()\n",
    "\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        for face_batch,y_batch in train_loader:\n",
    "            face_batch=face_batch.to(device)\n",
    "            y_batch=y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs=model(face_batch)\n",
    "\n",
    "            loss=criterion(outputs,y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "            _,predicted=torch.max(outputs,1)\n",
    "            correct+=(predicted==y_batch).sum().item()\n",
    "            total+=y_batch.size(0)\n",
    "        \n",
    "        acc=100*correct/total\n",
    "        tabTrain.append(acc)\n",
    "        print(f\"\\nEpoch : {epoch+1}/{nb_epoch}\\n Perte :{total_loss:.4f}\\n Accuracy : {acc:.2f}%\")\n",
    "\n",
    "        if val_split:\n",
    "            true,false,tot=resnetEval(model,loader=val_loader,labels=labels,graph=False)\n",
    "            val_acc=sum(true.values())/sum(tot.values())\n",
    "            tabVal.append(100*val_acc)\n",
    "            if val_acc>best_val_acc:\n",
    "                best_val_acc=val_acc #si l'acc de la val est meilleure on l'assigne et on reset le timer \n",
    "                wait=0\n",
    "                best_model_faces=copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                wait+=1\n",
    "                if wait>=patience: #on attend le nombre d'epoch max sans amélioration\n",
    "                    print(\"Early stopping activated\")\n",
    "                    break\n",
    "            print(f\"\\n Val accuracy : {val_acc*100:.2f}%\\n Série sans amélioration : {wait}\")\n",
    "    model.load_state_dict(best_model_faces)\n",
    "        \n",
    "    absc=list(range(len(tabTrain)))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(absc,tabTrain,label=\"Training\",marker=\"o\")\n",
    "    if val_split:\n",
    "        plt.plot(absc,tabVal,label=\"Validation\",marker=\"s\")\n",
    "    plt.xlim(1,len(tabTrain))\n",
    "    plt.ylim(0,100)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Training terminé.\")\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d973e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetEval(model,loader,labels,graph=False):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    labelF=defaultdict(int)\n",
    "    labelC=defaultdict(int)\n",
    "    labelT=defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for faces_batch,y_batch in loader:\n",
    "            faces_batch=faces_batch.to(device)\n",
    "            y_batch=y_batch.to(device)\n",
    "\n",
    "            outputs=model(faces_batch)\n",
    "\n",
    "            _,pred=torch.max(outputs,1)\n",
    "            for i in range(len(y_batch)):\n",
    "                label=y_batch[i].item()\n",
    "                pred[i]=pred[i].item()\n",
    "\n",
    "                if pred[i]==label:\n",
    "                    labelC[label]+=1\n",
    "                else:\n",
    "                    labelF[label]+=1\n",
    "                labelT[label]+=1\n",
    "\n",
    "    if graph:\n",
    "        print(f\"Total accuracy : {sum(labelC.values())/sum(labelT.values())*100:.2f}%\")\n",
    "        precisions={}\n",
    "        for label in labelT:\n",
    "            tot=labelT[label]\n",
    "            correct=labelC.get(label,0)\n",
    "            precisions[label]=correct/tot if tot>0 else 0\n",
    "        prec_sorted=sorted(precisions.items(),key=lambda x: x[1])\n",
    "        labels_ids,values= zip(*prec_sorted)\n",
    "\n",
    "        id2label = {v: k for k, v in labels.items()}\n",
    "        class_names=[id2label[i] for i in labels_ids]\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(class_names, values, color='skyblue')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Accuracy par classe\")\n",
    "        plt.title(\"Performance du modèle par classe\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return labelC,labelF,labelT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a089972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sample : 17534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=extract_data(\"DataFaces\")\n",
    "labels={label:i for i,label in enumerate(sorted(set(y_train)))}\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_train,y_train,test_size=0.2,random_state=1,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb89845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asami\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\asami\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1/50\n",
      " Perte :1597.1509\n",
      " Accuracy : 2.84%\n",
      "\n",
      " Val accuracy : 6.45%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 2/50\n",
      " Perte :1503.3181\n",
      " Accuracy : 7.44%\n",
      "\n",
      " Val accuracy : 10.26%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 3/50\n",
      " Perte :1440.9698\n",
      " Accuracy : 10.56%\n",
      "\n",
      " Val accuracy : 12.97%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 4/50\n",
      " Perte :1393.6657\n",
      " Accuracy : 12.82%\n",
      "\n",
      " Val accuracy : 14.08%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 5/50\n",
      " Perte :1354.9157\n",
      " Accuracy : 14.44%\n",
      "\n",
      " Val accuracy : 15.00%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 6/50\n",
      " Perte :1325.5957\n",
      " Accuracy : 15.57%\n",
      "\n",
      " Val accuracy : 16.32%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 7/50\n",
      " Perte :1301.1274\n",
      " Accuracy : 16.80%\n",
      "\n",
      " Val accuracy : 16.22%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 8/50\n",
      " Perte :1279.7760\n",
      " Accuracy : 17.57%\n",
      "\n",
      " Val accuracy : 17.07%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 9/50\n",
      " Perte :1257.4443\n",
      " Accuracy : 18.67%\n",
      "\n",
      " Val accuracy : 18.32%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 10/50\n",
      " Perte :1244.0743\n",
      " Accuracy : 18.97%\n",
      "\n",
      " Val accuracy : 18.82%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 11/50\n",
      " Perte :1225.3537\n",
      " Accuracy : 20.06%\n",
      "\n",
      " Val accuracy : 18.50%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 12/50\n",
      " Perte :1216.1174\n",
      " Accuracy : 20.44%\n",
      "\n",
      " Val accuracy : 20.10%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 13/50\n",
      " Perte :1202.9084\n",
      " Accuracy : 20.93%\n",
      "\n",
      " Val accuracy : 20.10%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 14/50\n",
      " Perte :1198.0921\n",
      " Accuracy : 20.52%\n",
      "\n",
      " Val accuracy : 20.17%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 15/50\n",
      " Perte :1180.2538\n",
      " Accuracy : 21.84%\n",
      "\n",
      " Val accuracy : 20.60%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 16/50\n",
      " Perte :1175.5068\n",
      " Accuracy : 22.12%\n",
      "\n",
      " Val accuracy : 20.35%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 17/50\n",
      " Perte :1171.7878\n",
      " Accuracy : 22.56%\n",
      "\n",
      " Val accuracy : 21.06%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 18/50\n",
      " Perte :1164.2377\n",
      " Accuracy : 22.52%\n",
      "\n",
      " Val accuracy : 20.99%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 19/50\n",
      " Perte :1149.4975\n",
      " Accuracy : 23.31%\n",
      "\n",
      " Val accuracy : 21.31%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 20/50\n",
      " Perte :1151.1222\n",
      " Accuracy : 22.69%\n",
      "\n",
      " Val accuracy : 21.20%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 21/50\n",
      " Perte :1142.2456\n",
      " Accuracy : 23.64%\n",
      "\n",
      " Val accuracy : 21.77%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 22/50\n",
      " Perte :1130.2401\n",
      " Accuracy : 24.14%\n",
      "\n",
      " Val accuracy : 22.31%\n",
      " Série sans amélioration : 0\n",
      "\n",
      "Epoch : 23/50\n",
      " Perte :1130.1952\n",
      " Accuracy : 23.97%\n",
      "\n",
      " Val accuracy : 21.31%\n",
      " Série sans amélioration : 1\n",
      "\n",
      "Epoch : 24/50\n",
      " Perte :1125.6265\n",
      " Accuracy : 24.27%\n",
      "\n",
      " Val accuracy : 22.31%\n",
      " Série sans amélioration : 2\n"
     ]
    }
   ],
   "source": [
    "model=resnetTrain(X_train,y_train,nb_epoch=50,labels=labels,batch_size=32,patience=10,val_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305ed87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Image enregistrée : 18\n",
      "19\n",
      "Image enregistrée : 19\n",
      "20\n",
      "Image enregistrée : 20\n",
      "21\n",
      "Image enregistrée : 21\n",
      "22\n",
      "Image enregistrée : 22\n",
      "23\n",
      "Image enregistrée : 23\n",
      "24\n",
      "Image enregistrée : 24\n",
      "25\n",
      "Image enregistrée : 25\n",
      "26\n",
      "Image enregistrée : 26\n",
      "27\n",
      "Image enregistrée : 27\n",
      "28\n",
      "Image enregistrée : 28\n",
      "29\n",
      "Image enregistrée : 29\n",
      "30\n",
      "Image enregistrée : 30\n",
      "31\n",
      "Image enregistrée : 31\n",
      "32\n",
      "Image enregistrée : 32\n",
      "33\n",
      "Image enregistrée : 33\n",
      "34\n",
      "Image enregistrée : 34\n",
      "35\n",
      "Image enregistrée : 35\n",
      "36\n",
      "Image enregistrée : 36\n",
      "37\n",
      "Image enregistrée : 37\n",
      "38\n",
      "Image enregistrée : 38\n",
      "39\n",
      "Image enregistrée : 39\n",
      "40\n",
      "Image enregistrée : 40\n",
      "41\n",
      "Image enregistrée : 41\n",
      "42\n",
      "Image enregistrée : 42\n",
      "43\n",
      "Image enregistrée : 43\n",
      "44\n",
      "Image enregistrée : 44\n",
      "45\n",
      "Image enregistrée : 45\n",
      "46\n",
      "Image enregistrée : 46\n",
      "47\n",
      "Image enregistrée : 47\n",
      "48\n",
      "Image enregistrée : 48\n",
      "49\n",
      "Image enregistrée : 49\n",
      "50\n",
      "Image enregistrée : 50\n",
      "51\n",
      "Image enregistrée : 51\n",
      "52\n",
      "Image enregistrée : 52\n",
      "53\n",
      "Image enregistrée : 53\n",
      "54\n",
      "Image enregistrée : 54\n",
      "55\n",
      "Image enregistrée : 55\n",
      "56\n",
      "Image enregistrée : 56\n",
      "57\n",
      "Image enregistrée : 57\n",
      "58\n",
      "Image enregistrée : 58\n",
      "59\n",
      "Image enregistrée : 59\n",
      "60\n",
      "Image enregistrée : 60\n",
      "61\n",
      "Image enregistrée : 61\n",
      "62\n",
      "Image enregistrée : 62\n",
      "63\n",
      "Image enregistrée : 63\n",
      "64\n",
      "Image enregistrée : 64\n",
      "65\n",
      "Image enregistrée : 65\n",
      "66\n",
      "Image enregistrée : 66\n",
      "67\n",
      "Image enregistrée : 67\n",
      "68\n",
      "Image enregistrée : 68\n",
      "69\n",
      "Image enregistrée : 69\n",
      "70\n",
      "Image enregistrée : 70\n",
      "71\n",
      "Image enregistrée : 71\n",
      "72\n",
      "Image enregistrée : 72\n",
      "73\n",
      "Image enregistrée : 73\n",
      "74\n",
      "Image enregistrée : 74\n",
      "75\n",
      "Image enregistrée : 75\n",
      "76\n",
      "Image enregistrée : 76\n",
      "77\n",
      "Image enregistrée : 77\n",
      "78\n",
      "Image enregistrée : 78\n",
      "Program exited\n"
     ]
    }
   ],
   "source": [
    "capture=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "u=0\n",
    "path=\"TestPhotos\"\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Erreur : Impossible d'ouvrir la webcam\")\n",
    "else:\n",
    "    while True:\n",
    "        ret,frame=capture.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #frame=cv2.rotate(frame,cv2.ROTATE_90_CLOCKWISE)\n",
    "        faces,eyes=extract_face_and_eyes(frame)\n",
    "        if len(faces)!=0:\n",
    "            u+=1\n",
    "            for i,face in enumerate(faces):\n",
    "                x,y,w,h=face\n",
    "                x2,y2,w2,h2=eyes[i]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,255),3)\n",
    "                cv2.rectangle(frame,(x2,y2),(x2+w2,y2+h2),(255,0,0),3)\n",
    "                if u>=60:\n",
    "                    u=0\n",
    "                    file=os.listdir(path)\n",
    "                    if f\"{path}/photo-{len(file)}\" not in file:\n",
    "                        cv2.imwrite(f\"{path}/photo-{len(file)}.jpg\",frame[y:y+h,x:x+w])\n",
    "                    print(f\"Image enregistrée : {len(file)}\")\n",
    "\n",
    "        cv2.imshow(\"Webcam\",frame)\n",
    "        if cv2.waitKey(1) & 0xFF==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Program exited\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
