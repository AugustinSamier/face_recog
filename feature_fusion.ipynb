{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894072b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from math import sqrt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f7162b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path,create_test=None):\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    file=os.listdir(path)\n",
    "\n",
    "    for entity in file:\n",
    "        try:\n",
    "            entityPath=os.listdir(f\"{path}/{entity}\")\n",
    "            for i,data in enumerate(entityPath):\n",
    "                try:\n",
    "                    current_img=cv2.imread(f\"{path}/{entity}/{data}\")\n",
    "                    if current_img is not None:\n",
    "                        if (create_test is not None and ((len(entityPath)>1) and (i==0))):\n",
    "                            X_test.append(current_img)\n",
    "                            y_test.append(entity)\n",
    "                        else:\n",
    "                            X_train.append(current_img)\n",
    "                            y_train.append(entity)\n",
    "                    else:\n",
    "                        print(f\"{path}/{entity}/{data} is None type\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur avec {path}/{entity}/{data} : {e}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Erreur avec {path}/{entity} : {ex}\")\n",
    "    \n",
    "    print(f\"Number of training sample : {len(X_train)}\\n\")\n",
    "    if create_test is not None:\n",
    "        print(f\"Number of test sample : {len(X_test)}\\n\")\n",
    "        return X_train,y_train,X_test,y_test\n",
    "    else:\n",
    "        return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e57c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_face_test=transforms.Compose([\n",
    "    transforms.ToPILImage(), #transforme en format PIL\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor() #reconvertit l'img en format tensor\n",
    "])\n",
    "\n",
    "transform_face=transforms.Compose([\n",
    "    transforms.ToPILImage(), #transforme en format PIL\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.RandomHorizontalFlip(), #0.5 de proba d'inverser la gauche et la droite de l'img pour rendre le modèle invariant à la symétrie\n",
    "    transforms.RandomRotation(15), #applique rota random entre -10° et +10° \n",
    "    transforms.ColorJitter(brightness=0.3,contrast=0.3,saturation=0.2,hue=0.02), #altère aléatoirement la luminosité et le contraste de l'image\n",
    "    transforms.ToTensor() #reconvertit l'img en format tensor\n",
    "])\n",
    "\n",
    "transform_eyes=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((50,50)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_eyes_test=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((50,50)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_and_eyes(img):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        img=cv2.imread(img)\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60,60)\n",
    "    )\n",
    "\n",
    "    if len(faces)==0:\n",
    "        return [],[]\n",
    "    \n",
    "    x,y,w,h=faces[0]\n",
    "    face=img[y:y+h,x:x+w]\n",
    "    eyes=eye_cascade.detectMultiScale(cv2.cvtColor(face,cv2.COLOR_BGR2GRAY))\n",
    "    if len(eyes)==2:\n",
    "        if eyes[0][0]<eyes[1][0]:\n",
    "            x2=eyes[0][0]\n",
    "            w2=eyes[1][0]+eyes[1][2]-x2\n",
    "        else:\n",
    "            x2=eyes[1][0]\n",
    "            w2=eyes[0][0]+eyes[0][2]-x2\n",
    "        if eyes[0][1]<eyes[1][1]:\n",
    "            y2=eyes[0][1]\n",
    "            h2=eyes[1][1]+eyes[1][3]-y2\n",
    "        else:\n",
    "            y2=eyes[1][1]\n",
    "            h2=eyes[0][1]+eyes[0][3]-y2\n",
    "        imgEyes=face[y2:y2+h2,x2:x2+w2]\n",
    "        return face,imgEyes\n",
    "    else:\n",
    "        return face,[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a295f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(X):\n",
    "    X_face=[]\n",
    "    X_eyes=[]\n",
    "    for i,x in enumerate(X):\n",
    "        face,eyes=extract_face_and_eyes(x)\n",
    "        X_face.append(face)\n",
    "        if len(eyes)!=0:\n",
    "            X_eyes.append(eyes)\n",
    "        else:\n",
    "            X_eyes.append(None)\n",
    "    return X_face,X_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72564b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEyesDataset(Dataset):\n",
    "    def __init__(self,faces,eyes,labels):\n",
    "        self.faces=faces\n",
    "        self.eyes=eyes\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.faces)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.faces[idx],self.eyes[idx],self.labels[idx]\n",
    "\n",
    "def fusion_train(X_train,y_train,nb_epoch,labels,batch_size,models=None,patience=5,val_split=False):\n",
    "    nb_classes=len(list(set(y_train)))\n",
    "    if not models:\n",
    "        facemodel,eyesmodel,fusionModel=feature_fusionModel(nb_classes)\n",
    "    else:\n",
    "        fusionModel,facemodel,eyesmodel=models\n",
    "    \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    fusionModel.to(device)\n",
    "    facemodel.to(device)\n",
    "    eyesmodel.to(device)\n",
    "\n",
    "    X_face,X_eyes=createData(X_train)\n",
    "    faceslist=([transform_face(x) for x in X_face])\n",
    "    ytorchlist=([labels[y] for y in y_train])\n",
    "    eyeslist=[]\n",
    "    for eye in X_eyes:\n",
    "        if eye:\n",
    "            eyeslist.append(transform_eyes(eye))\n",
    "        else:\n",
    "            eyeslist.append(torch.zeros((1,50,50)))\n",
    "\n",
    "    if val_split:\n",
    "        faceslist,faces_val,eyeslist,eyes_val,ytorchlist,y_val=train_test_split(faceslist,eyeslist,ytorchlist,test_size=0.2,random_state=1,stratify=ytorchlist)\n",
    "        faces_val=torch.stack(faces_val)\n",
    "        eyes_val=torch.stack(eyes_val)\n",
    "        y_val=torch.tensor(y_val,dtype=torch.long)\n",
    "        val_dataset=FaceEyesDataset(faces_val,eyes_val,y_val)\n",
    "        val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "        best_val_acc=0\n",
    "        wait=0\n",
    "\n",
    "    faces=torch.stack(faceslist)\n",
    "    eyes=torch.stack(eyeslist)\n",
    "    ytorch=torch.tensor(ytorchlist,dtype=torch.long)\n",
    "\n",
    "    train_dataset=FaceEyesDataset(faces,eyes,ytorch)\n",
    "    train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(list(facemodel.parameters())+list(eyesmodel.parameters())+list(fusionModel.parameters()), lr=0.001)\n",
    "    #on veut optimiser les poids des trois modèles en même temps\n",
    "    tabTrain=[]\n",
    "    tabVal=[]\n",
    "    for epoch in range(nb_epoch):\n",
    "        fusionModel.train()\n",
    "        facemodel.train()\n",
    "        eyesmodel.train()\n",
    "\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        for face_batch,eyes_batch,y_batch in train_loader:\n",
    "            face_batch=face_batch.to(device)\n",
    "            eye_batch=eye_batch.to(device)\n",
    "            y_batch=y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            featuresFaces=facemodel(face_batch)\n",
    "            featuresEyes=eyesmodel(eyes_batch)\n",
    "            featuresFusion=torch.cat((featuresFaces,featuresEyes),dim=1)\n",
    "            outputs=fusionModel(featuresFusion)\n",
    "\n",
    "            loss=criterion(outputs,y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "            _,predicted=torch.max(outputs,1)\n",
    "            correct+=(predicted==y_batch).sum().item()\n",
    "            total+=y_batch.size(0)\n",
    "        \n",
    "        acc=100*correct/total\n",
    "        tabTrain.append(acc)\n",
    "        print(f\"\\nEpoch : {epoch+1}/{nb_epoch}\\n Perte :{total_loss:.4f}\\n Précision : {acc:.2f}%\")\n",
    "\n",
    "        if val_split:\n",
    "            true,false,tot=fusionEvaluate((facemodel,eyesmodel,fusionModel),val_loader,labels)\n",
    "            val_acc=sum(true.values())/sum(tot.values())\n",
    "            tabVal.append(100*val_acc)\n",
    "            if val_acc>best_val_acc:\n",
    "                best_val_acc=val_acc #si l'acc de la val est meilleure on l'assigne et on reset le timer \n",
    "                wait=0\n",
    "                best_model_faces=copy.deepcopy(facemodel.state_dict())\n",
    "                best_model_eyes=copy.deepcopy(eyesmodel.state_dict())\n",
    "                best_model_fusion=copy.deepcopy(fusionModel.state_dict())\n",
    "            else:\n",
    "                if wait>=patience: #on attend le nombre d'epoch max sans amélioration\n",
    "                    print(\"Early stopping activated\")\n",
    "                    break\n",
    "                wait+=1\n",
    "            print(f\"\\n Val accuracy : {val_acc*100:.2f}%\\n Série sans amélioration : {wait}\")\n",
    "    facemodel.load_state_dict(best_model_faces)\n",
    "    eyesmodel.load_state_dict(best_model_eyes)\n",
    "    fusionModel.load_state_dict(best_model_fusion)\n",
    "        \n",
    "    absc=list(range(len(tabTrain)))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(absc,tabTrain,label=\"Training\",marker=\"o\")\n",
    "    if val_split is not None:\n",
    "        plt.plot(absc,tabVal,label=\"Validation\",marker=\"s\")\n",
    "    plt.xlim(1,len(tabTrain))\n",
    "    plt.ylim(0,100)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Training terminé.\")\n",
    "    plt.show()\n",
    "    return facemodel,eyesmodel,fusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "201b5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusionEvaluate(models,loader,labels,graph=False):\n",
    "    if device is None:\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    facemodel,eyesmodel,fusionmodel=models\n",
    "    facemodel.to(device)\n",
    "    facemodel.eval()\n",
    "    eyesmodel.to(device)\n",
    "    eyesmodel.eval()\n",
    "    fusionmodel.to(device)\n",
    "    fusionmodel.eval()\n",
    "\n",
    "    labelF=defaultdict(int)\n",
    "    labelC=defaultdict(int)\n",
    "    labelT=defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for faces_batch,eyes_batch,y_batch in loader:\n",
    "            faces_batch=faces_batch.to(device)\n",
    "            eyes_batch=eyes_batch.to(device)\n",
    "            y_batch=y_batch.to(device)\n",
    "\n",
    "            featuresFaces=facemodel(faces_batch)\n",
    "            featuresEyes=eyesmodel(eyes_batch)\n",
    "            featuresFusion=torch.cat((featuresFaces,featuresEyes),dim=1)\n",
    "            outputs=fusionmodel(featuresFusion)\n",
    "\n",
    "            _,pred=torch.max(outputs,1)\n",
    "            for i in range(len(y_batch)):\n",
    "                label=y_batch[i].item()\n",
    "                pred[i]=pred[i].item()\n",
    "\n",
    "                if pred[i]==label:\n",
    "                    labelC[label]+=1\n",
    "                else:\n",
    "                    labelF[label]+=1\n",
    "                labelT[label]+=1\n",
    "\n",
    "    if graph:\n",
    "        print(f\"Total precision : {sum(labelC.values())/sum(labelT.values())*100:.2f}%\")\n",
    "        precisions={}\n",
    "        for label in labelT:\n",
    "            tot=labelT[label]\n",
    "            correct=labelC.get(label,0)\n",
    "            precisions[label]=correct/tot if tot>0 else 0\n",
    "        prec_sorted=sorted(precisions.items(),key=lambda x: x[1])\n",
    "        labels_ids,values= zip(*prec_sorted)\n",
    "\n",
    "        id2label = {v: k for k, v in labels.items()}\n",
    "        class_names=[id2label[i] for i in labels_ids]\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(class_names, values, color='skyblue')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Précision par classe\")\n",
    "        plt.title(\"Performance du modèle par classe\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return labelC,labelF,labelT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ba4c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fusionModel(nb_classes):\n",
    "    face_model=nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),   # (1,100,100) -> (16,100,100)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),                           # (16,50,50)\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),  # (32,50,50)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),                           # (32,25,25)\n",
    "\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (64,25,25)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),                           # (64,12,12)\n",
    "\n",
    "        nn.Flatten(),                                 # 64*12*12 = 9216\n",
    "        nn.Linear(9216, 512),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    eyes_model=nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),   # (1,50,50) -> (16,50,50)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),                           # (16,25,25)\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),  # (32,25,25)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),                           # (32,12,12)\n",
    "\n",
    "        nn.Flatten(),                                 # 32*12*12 = 4608\n",
    "        nn.Linear(4608, 256),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    final_classifier=nn.Sequential(\n",
    "        nn.Linear(768, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),  # Dropout pour éviter l'overfitting\n",
    "\n",
    "        nn.Linear(256, nb_classes)\n",
    "    )\n",
    "\n",
    "    return face_model,eyes_model,final_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a846921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sample : 17534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=extract_data(\"DataFaces\")\n",
    "labels={label:i for i,label in enumerate(sorted(set(y_train)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07623350",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_train,y_train,test_size=0.2,random_state=1,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac68eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m facemodel,eyesmodel,fusionmodel\u001b[38;5;241m=\u001b[39m\u001b[43mfusion_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m, in \u001b[0;36mfusion_train\u001b[0;34m(X_train, y_train, nb_epoch, labels, batch_size, models, patience, val_split)\u001b[0m\n\u001b[1;32m     23\u001b[0m eyesmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m X_face,X_eyes\u001b[38;5;241m=\u001b[39mcreateData(X_train)\n\u001b[0;32m---> 26\u001b[0m faceslist\u001b[38;5;241m=\u001b[39m([transform_face(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_face])\n\u001b[1;32m     27\u001b[0m ytorchlist\u001b[38;5;241m=\u001b[39m([labels[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_train])\n\u001b[1;32m     28\u001b[0m eyeslist\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m eyesmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m X_face,X_eyes\u001b[38;5;241m=\u001b[39mcreateData(X_train)\n\u001b[0;32m---> 26\u001b[0m faceslist\u001b[38;5;241m=\u001b[39m([\u001b[43mtransform_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_face])\n\u001b[1;32m     27\u001b[0m ytorchlist\u001b[38;5;241m=\u001b[39m([labels[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_train])\n\u001b[1;32m     28\u001b[0m eyeslist\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/transforms/functional.py:268\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    266\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic\u001b[38;5;241m.\u001b[39mnumpy(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be Tensor or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# if 2D image, add channel dimension (HWC)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     pic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(pic, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'list'>."
     ]
    }
   ],
   "source": [
    "facemodel,eyesmodel,fusionmodel=fusion_train(X_train,y_train,100,labels,32,patience=5,val_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "facestest,eyestest=extract_face_and_eyes(X_test)\n",
    "labelC,labelF,labelT=fusionEvaluate((facemodel,eyesmodel,fusionmodel),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusionPipeline(X,y):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
