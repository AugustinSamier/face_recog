{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44616c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@610.031] global persistence.cpp:566 open Can't open file: '/usr/local/lib/python3.10/site-packages/cv2/data/haarcascade_mcs_mouth.xml' in read mode\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset,random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "mouth_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_mouth.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a8b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path,create_test=None):\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    file=os.listdir(path)\n",
    "\n",
    "    for entity in file:\n",
    "        try:\n",
    "            entityPath=os.listdir(f\"{path}/{entity}\")\n",
    "            for i,data in enumerate(entityPath):\n",
    "                try:\n",
    "                    current_img=cv2.imread(f\"{path}/{entity}/{data}\")\n",
    "                    if current_img is not None:\n",
    "                        if (create_test is not None and ((len(entityPath)>1) and (i==0))):\n",
    "                            X_test.append(current_img)\n",
    "                            y_test.append(entity)\n",
    "                        else:\n",
    "                            X_train.append(current_img)\n",
    "                            y_train.append(entity)\n",
    "                    else:\n",
    "                        print(f\"{path}/{entity}/{data} is None type\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur avec {path}/{entity}/{data} : {e}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Erreur avec {path}/{entity} : {ex}\")\n",
    "    \n",
    "    print(f\"Number of training sample : {len(X_train)}\\n\")\n",
    "    if create_test is not None:\n",
    "        print(f\"Number of test sample : {len(X_test)}\\n\")\n",
    "        return X_train,y_train,X_test,y_test\n",
    "    else:\n",
    "        return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad3e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_and_eyes(img):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        img=cv2.imread(img)\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60,60)\n",
    "    )\n",
    "\n",
    "    if len(faces)==0:\n",
    "        return [],[]\n",
    "    else:\n",
    "        trueFaces=[]\n",
    "        trueEyes=[]\n",
    "        for i,face in enumerate(faces):\n",
    "            x,y,w,h=face\n",
    "            imgFace=img[y:y+h,x:x+w]\n",
    "            eyes=eye_cascade.detectMultiScale(imgFace)\n",
    "            if len(eyes)>=2:\n",
    "                trueFaces.append(face)\n",
    "                if eyes[0][0]<eyes[1][0]:\n",
    "                    x2=eyes[0][0]\n",
    "                    w2=eyes[1][0]+eyes[1][2]-x2\n",
    "                else:\n",
    "                    x2=eyes[1][0]\n",
    "                    w2=eyes[0][0]+eyes[0][2]-x2\n",
    "                if eyes[0][1]<eyes[1][1]:\n",
    "                    y2=eyes[0][1]\n",
    "                    h2=eyes[1][1]+eyes[1][3]-y2\n",
    "                else:\n",
    "                    y2=eyes[1][1]\n",
    "                    h2=eyes[0][1]+eyes[0][3]-y2\n",
    "                trueEyes.append((x2+x,y2+y,w2,h2))\n",
    "        return trueFaces,trueEyes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f728e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(img):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        img=cv2.imread(img) #si on passe un path Ã§a read l'img\n",
    "\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60,60)\n",
    "    )\n",
    "\n",
    "    if len(faces)==0:\n",
    "        return img #retourne l'img si aucun visage n'est detectÃ©\n",
    "    \n",
    "    #si je veux detecter tous les visages prÃ©sents je dois faire une boucle for x,y,w,h in faces: et return une liste d'img crops\n",
    "    x,y,w,h=faces[0] #faces retourne n tuples (x,y,h,w) correspondants aux coords du visage\n",
    "    face=img[y:y+h,x:x+w] #retourne l'img crop aux coords detectÃ©es\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee171cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sample : 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=extract_data(\"nous/Train\")\n",
    "X_faces=[]\n",
    "for img in X_train:\n",
    "    X_faces.append(extract_face(img))\n",
    "\n",
    "labels={label:i for i,label in enumerate(sorted(set(y_train)))}\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_faces,y_train,test_size=0.2,random_state=1,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d38458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # Moyennes ImageNet\n",
    "                         [0.229, 0.224, 0.225])  # Ã‰carts-types ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcc8a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        # Conversion image\n",
    "        image = Image.fromarray(image.astype(np.uint8))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # ðŸ” Convertir label en tensor (si ce n'est pas dÃ©jÃ  le cas)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b56430",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)  # â†’ array d'entiers\n",
    "dataset = FaceDataset(X_faces, y_train_encoded, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f94957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Geler les couches convolutionnelles\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Adapter la derniÃ¨re couche (classification)\n",
    "num_classes = 2\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50ccc4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7389\n",
      "Epoch 2, Loss: 0.7173\n",
      "Epoch 3, Loss: 0.6696\n",
      "Epoch 4, Loss: 0.5453\n",
      "Epoch 5, Loss: 0.4666\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)  # On n'entraÃ®ne que la fc\n",
    "\n",
    "# Boucle d'entraÃ®nement\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b3362ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sample : 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test=extract_data(\"CameraFaces\")\n",
    "X_facesTest=[]\n",
    "for img in X_test:\n",
    "    X_facesTest.append(extract_face(img))\n",
    "\n",
    "labels={label:i for i,label in enumerate(sorted(set(y_test)))}\n",
    "le = LabelEncoder()\n",
    "y_test_enc = le.fit_transform(y_test)  # â†’ array d'entiers\n",
    "datasetTest = FaceDataset(X_facesTest, y_test_enc, transform=transform)\n",
    "test_loader = DataLoader(datasetTest, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73c87655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, 1) \n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7213c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resNet50V1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
