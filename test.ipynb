{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065c9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import face_recognition\n",
    "from datetime import datetime\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b67b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur avec Faces/pairs.txt : [Errno 20] Not a directory: 'Faces/pairs.txt'\n",
      "Erreur avec Faces/pairs_01.txt : [Errno 20] Not a directory: 'Faces/pairs_01.txt'\n",
      "Erreur avec Faces/pairs_02.txt : [Errno 20] Not a directory: 'Faces/pairs_02.txt'\n",
      "Erreur avec Faces/pairs_03.txt : [Errno 20] Not a directory: 'Faces/pairs_03.txt'\n",
      "Erreur avec Faces/pairs_04.txt : [Errno 20] Not a directory: 'Faces/pairs_04.txt'\n",
      "Erreur avec Faces/pairs_05.txt : [Errno 20] Not a directory: 'Faces/pairs_05.txt'\n",
      "Erreur avec Faces/pairs_06.txt : [Errno 20] Not a directory: 'Faces/pairs_06.txt'\n",
      "Erreur avec Faces/pairs_07.txt : [Errno 20] Not a directory: 'Faces/pairs_07.txt'\n",
      "Erreur avec Faces/pairs_08.txt : [Errno 20] Not a directory: 'Faces/pairs_08.txt'\n",
      "Erreur avec Faces/pairs_09.txt : [Errno 20] Not a directory: 'Faces/pairs_09.txt'\n",
      "Erreur avec Faces/pairs_10.txt : [Errno 20] Not a directory: 'Faces/pairs_10.txt'\n"
     ]
    }
   ],
   "source": [
    "faces_images=[]\n",
    "faces_labels=[]\n",
    "\n",
    "faces_file=os.listdir(\"Faces\")\n",
    "\n",
    "for person in faces_file:\n",
    "    try:\n",
    "        faces=os.listdir(f\"Faces/{person}\")\n",
    "        for face in faces:\n",
    "            try:\n",
    "                current_face=cv2.imread(f\"Faces/{person}/{face}\")\n",
    "                if current_face is not None:\n",
    "                    faces_images.append(current_face)\n",
    "                    faces_labels.append(person)\n",
    "                else:\n",
    "                    print(f\"Faces/{person}/{current_face} is None type\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur avec Faces/{person}/{current_face} : {e}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Erreur avec Faces/{person} : {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12b56c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgProcess(img):\n",
    "    try:\n",
    "        img.dtype\n",
    "    except:\n",
    "        print(\"convertion de l'img\")\n",
    "        img=cv2.imread(img)\n",
    "    gris=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #RGB -> Gray -> dim(255,255,3)=255*255*3 val de couleur -> dim(255,255,1) -> 255*255*1 val de couleur\n",
    "    resize=cv2.resize(gris,(100,100)) #dim -> dim(100,100,1) -> 100*100*1 = 10000 val de couleur\n",
    "    flat=resize.flatten() #2D -> 1D vec de (10000,1)\n",
    "    return flat\n",
    "\n",
    "processed_faces=[]\n",
    "for img in faces_images:\n",
    "    processed_faces.append(imgProcess(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distEuclid(img1,img2): #p1(a1,a2,a3...,a10000) p2(b1,b2,b3...,b10000) car chaque image a 10000 val de couleur\n",
    "    img1 = img1.astype(np.int32) #pour avoir des valeur négatives sinon en uint8 c'est de 0 à 255 (pas negatif)\n",
    "    img2 = img2.astype(np.int32)\n",
    "\n",
    "    #distance euclidienne entre deux points (en 2D) p1(x1,y1) et p2(x2,y2): d=hypothénuse du triangle rectangle\n",
    "    #ainsi d²=a²+b² et d=sqrt(a²+b²) avec a=abs(x1-x2) et b=abs(y1-y2) DONC -> d=sqrt((x1-x2)²+(y1-y2)²)\n",
    "    #en n dim : d=sqrt((a1-b1)²+(a2-b2)²...+(an-bn)²) = sqrt(sum[1;N]((a-b)²))\n",
    "    return sqrt(sum((a-b)**2 for a,b in zip(img1,img2))) #zip -> pour deux listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dcc79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train,y_train, newImg,k=3):\n",
    "    distances=[]\n",
    "    for i in range(len(X_train)):\n",
    "        dist=distEuclid(X_train[i],newImg) #on calcule la distance entre la nouvelle image et toutes les autres images\n",
    "        distances.append((dist,y_train[i])) #on stock la distance avec le label de chaque image à laquelle on a calculé la distance\n",
    "    \n",
    "    distances.sort(key=lambda x: x[0]) #on trie la liste de distances dans l'ordre croissant comme ça les derniers termes sont les plus proche de la nouvelle image\n",
    "    k_voisins=distances[:k] #on prend les k plus proches voisins\n",
    "\n",
    "    counts={} #dictionnaire pour compter les labels les plus proches et leur occurences\n",
    "    for _,label in k_voisins: #dans le tableau c'est [distance,label], ici on veut que le label donc _\n",
    "        counts[label]=counts.get(label,0)+1 #.get(a,b) = prend la val a si existe sinon prend la val b\n",
    "\n",
    "    return max(counts,key=counts.get) #compare les valeurs mais renvoie le label du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa94afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convertion de l'img\n",
      "Aaron_Eckhart\n"
     ]
    }
   ],
   "source": [
    "print(knn_predict(processed_faces,faces_labels,imgProcess(\"test.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea225bcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         encoding_list\u001b[38;5;241m.\u001b[39mappend(face_encoding)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding_list\n\u001b[0;32m----> 9\u001b[0m known_face_encodings\u001b[38;5;241m=\u001b[39m\u001b[43mget_face_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaces_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mget_face_encodings\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n\u001b[1;32m      4\u001b[0m     imageE\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcvtColor(image,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m----> 5\u001b[0m     face_encoding\u001b[38;5;241m=\u001b[39m\u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     encoding_list\u001b[38;5;241m.\u001b[39mappend(face_encoding)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoding_list\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/face_recognition/api.py:213\u001b[0m, in \u001b[0;36mface_encodings\u001b[0;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mface_encodings\u001b[39m(face_image, known_face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_jitters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Given an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    :return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     raw_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_face_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/face_recognition/api.py:156\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[0;34m(face_image, face_locations, model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_raw_face_landmarks\u001b[39m(face_image, face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m face_locations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m [_css_to_rect(face_location) \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/face_recognition/api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "def get_face_encodings(images):\n",
    "    encoding_list=[]\n",
    "    for i,image in enumerate(images):\n",
    "        imageE=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        face_encoding=face_recognition.face_encodings(imageE)[0]\n",
    "        encoding_list.append(face_encoding)\n",
    "    return encoding_list\n",
    "\n",
    "known_face_encodings=get_face_encodings(faces_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_recognised_face(name,filename=\"records.csv\"):\n",
    "    capture_date=datetime.now().strftime(\"%Y-%m-d\")\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename,\"w\") as f:\n",
    "            f.write(\"Name,Date,Time\")\n",
    "        \n",
    "    with open(filename,\"r+\") as file:\n",
    "        lines=file.readlines()\n",
    "        existing_names=[line.split(\",\")[0] for line in lines]\n",
    "        if name not in existing_names:\n",
    "            now=datetime.now()\n",
    "            current_time=now.strftime(\"%H:%M:%S\")\n",
    "            file.write(f\"\\n{name},{capture_date},{current_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_recognition_program():\n",
    "    video_capture=cv2.VideoCapture(1)\n",
    "    while True:\n",
    "        frame=video_capture.read()\n",
    "        if frame is not None:\n",
    "            frame=frame[1]\n",
    "            resized_frame=cv2.resize(frame,(0,0),None,0.25,0.25)\n",
    "            resized_frame=cv2.cvtColor(resized_frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            face_locations=face_recognition.face_locations(resized_frame)\n",
    "            current_face_encodings=face_recognition.face_encodings(resized_frame,face_locations)\n",
    "            for face_encoding, location in zip(current_face_encodings,face_locations):\n",
    "                matches=face_recognition.compare_faces(known_face_encodings,face_encoding)\n",
    "                face_distances=face_recognition.face_distance(known_face_encodings,face_encoding)\n",
    "                best_match_index=np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    recognized_name=faces_labels[best_match_index].upper()\n",
    "                    top,right,bottom,left=location\n",
    "                    top,right,bottom,left=top*4,right*4,bottom*4,left*4\n",
    "                    cv2.rectangle(frame,(left,top),(right,bottom),(0,255,0),2)\n",
    "                    cv2.rectangle(frame,(left,bottom-35),(right,bottom),(0.255,0),cv2.FILLED)\n",
    "                    cv2.putText(frame,recognized_name,(left+6,bottom-6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2)\n",
    "                    document_recognised_face(recognized_name)\n",
    "            cv2.imshow(\"Webcam\",frame)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key==ord(\"q\"):\n",
    "            break\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10842a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.title(\"Face Recognition Program\")\n",
    "\n",
    "label=tk.Label(root,text=\"Click the button to start the facial recognition program\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "start_button=tk.Button(root,text=\"Start Recognition\",command=start_recognition_program)\n",
    "start_button.pack(pady=10)\n",
    "\n",
    "def quit_app():\n",
    "    root.quit()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "exit_button=tk.Button(root,text=\"Close\",command=quit_app)\n",
    "exit_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
