{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from math import sqrt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cb23f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path,create_test=None):\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    file=os.listdir(path)\n",
    "\n",
    "    for entity in file:\n",
    "        try:\n",
    "            entityPath=os.listdir(f\"{path}/{entity}\")\n",
    "            for i,data in enumerate(entityPath):\n",
    "                try:\n",
    "                    current_img=cv2.imread(f\"{path}/{entity}/{data}\")\n",
    "                    if current_img is not None:\n",
    "                        if (create_test is not None and ((len(entityPath)>1) and (i==0))):\n",
    "                            X_test.append(current_img)\n",
    "                            y_test.append(entity)\n",
    "                        else:\n",
    "                            X_train.append(current_img)\n",
    "                            y_train.append(entity)\n",
    "                    else:\n",
    "                        print(f\"{path}/{entity}/{data} is None type\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur avec {path}/{entity}/{data} : {e}\")\n",
    "        except Exception as ex:\n",
    "            print(f\"Erreur avec {path}/{entity} : {ex}\")\n",
    "    \n",
    "    print(f\"Number of training sample : {len(X_train)}\\n\")\n",
    "    if create_test is not None:\n",
    "        print(f\"Number of test sample : {len(X_test)}\\n\")\n",
    "        return X_train,y_train,X_test,y_test\n",
    "    else:\n",
    "        return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12b56c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgProcess(img,withTorch=None):\n",
    "    if not isinstance(img,np.ndarray):\n",
    "        print(\"convertion de l'img\")\n",
    "        img=cv2.imread(img)\n",
    "    \n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #RGB -> Gray -> dim(255,255,3)=255*255*3 val de couleur = 195075 vals -> dim(255,255,1) -> 255*255*1 val de couleur = 65025 vals pour l'exemple du 255*255\n",
    "    img=cv2.resize(img,(100,100)) #dim -> dim(100,100,1) -> 100*100*1 = 10000 val de couleur\n",
    "    \n",
    "    if withTorch is None:\n",
    "        img=img.flatten() #2D -> 1D vec de (10000,1)\n",
    "    else:\n",
    "        img=transform(img)\n",
    "        img=img.unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7cbfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distEuclid(img1,img2): #p1(a1,a2,a3...,a10000) p2(b1,b2,b3...,b10000) car chaque image a 10000 val de couleur\n",
    "    #img1 = img1.astype(np.int32) #pour avoir des valeur négatives sinon en uint8 c'est de 0 à 255 (pas negatif)\n",
    "    #img2 = img2.astype(np.int32)\n",
    "    diff=img1.astype(np.int32)-img2.astype(np.int32) #opti\n",
    "\n",
    "    #distance euclidienne entre deux points (en 2D) p1(x1,y1) et p2(x2,y2): d=hypothénuse du triangle rectangle\n",
    "    #ainsi d²=a²+b² et d=sqrt(a²+b²) avec a=abs(x1-x2) et b=abs(y1-y2) DONC -> d=sqrt((x1-x2)²+(y1-y2)²)\n",
    "    #en n dim : d=sqrt((a1-b1)²+(a2-b2)²...+(an-bn)²) = sqrt(sum[1;N]((a-b)²))\n",
    "    #return sqrt(sum((a-b)**2 for a,b in zip(img1,img2))) #zip -> pour deux listes\n",
    "    return np.sqrt(np.sum(diff**2)) #opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7dcc79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train,y_train, newImg,k):\n",
    "    distances=[]\n",
    "    for i in range(len(X_train)):\n",
    "        dist=distEuclid(X_train[i],newImg) #on calcule la distance entre la nouvelle image et toutes les autres images\n",
    "        distances.append((dist,y_train[i])) #on stock la distance avec le label de chaque image à laquelle on a calculé la distance\n",
    "    \n",
    "    distances.sort(key=lambda x: x[0]) #on trie la liste de distances dans l'ordre croissant comme ça les derniers termes sont les plus proche de la nouvelle image\n",
    "    k_voisins=distances[:k] #on prend les k plus proches voisins\n",
    "\n",
    "    counts={} #dictionnaire pour compter les labels les plus proches et leur occurences\n",
    "    for _,label in k_voisins: #dans le tableau c'est [distance,label], ici on veut que le label donc _\n",
    "        counts[label]=counts.get(label,0)+1 #.get(a,b) = prend la val a si existe sinon prend la val b\n",
    "\n",
    "    return max(counts,key=counts.get) #compare les valeurs mais renvoie le label du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6edd2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_evaluate(processed_train,y_train,processed_test,y_test,k,n):\n",
    "    knn_results=[]\n",
    "    true=0\n",
    "    if n is None:\n",
    "        n=len(processed_test)\n",
    "\n",
    "    print(f\"Number of sample set to : {n}\")\n",
    "    \n",
    "    for i,img in enumerate(processed_test[:n]):\n",
    "        print(f\"Image KNN : {i}\")\n",
    "        knn_results.append(knn_predict(processed_train,y_train,processed_test[i],k))\n",
    "        if knn_results[i]==y_test[i]:\n",
    "            true+=1\n",
    "    print(f\"Accuracy (%) : {100*true/n}\")\n",
    "\n",
    "    return knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5a05402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train,y_train,X_test,y_test,k=3,n=None):\n",
    "    processed_train=[]\n",
    "    for img in X_train:\n",
    "        processed_train.append(imgProcess(img))\n",
    "\n",
    "    processed_test=[]\n",
    "    for img in X_test:\n",
    "        processed_test.append(imgProcess(img))\n",
    "    print(\"Processing done.\\n\")\n",
    "\n",
    "    knn_result=knn_evaluate(processed_train,y_train,processed_test,y_test,k,n)\n",
    "    return knn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79fb2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgPadding(img,padding):\n",
    "    size=img.shape[0]\n",
    "    if padding is not None:\n",
    "        size=size+2*padding\n",
    "        newImg=np.zeros((size,size,3),dtype=np.uint8)\n",
    "        for x in range(padding,size-padding):\n",
    "            for y in range(padding,size-padding):\n",
    "                newImg[x][y]=img[x-padding][y-padding]\n",
    "    else:\n",
    "        newImg=img\n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4366c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(img,dim=2,padding=None,stride=1):\n",
    "    if padding is not None:\n",
    "        img=imgPadding(img,padding)\n",
    "    \n",
    "    newDim=((img.shape[0]-dim)//stride)+1\n",
    "    newImg=np.zeros([newDim,newDim,3])\n",
    "    \n",
    "    for rgb in range(img.shape[2]):\n",
    "        for x in range(0,(img.shape[0]//dim)*dim,stride):\n",
    "            for y in range(0,(img.shape[0]//dim)*dim,stride):\n",
    "                max=0\n",
    "                for h in range(dim):\n",
    "                    for l in range(dim):\n",
    "                        if img[x+l][y+h][rgb]>max:\n",
    "                            max=img[x+l][y+h][rgb]\n",
    "                newImg[((x-dim)//stride)+1][((y-dim)//stride)+1][rgb]=max\n",
    "    \n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3c230e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pooling(img,dim=2,padding=None,stride=1):\n",
    "    if padding is not None:\n",
    "        img=imgPadding(img,padding)\n",
    "    \n",
    "    newDim=((img.shape[0]-dim)//stride)+1\n",
    "    newImg=np.zeros([newDim,newDim,3])\n",
    "    \n",
    "    for rgb in range(img.shape[2]):\n",
    "        for x in range(0,(img.shape[0]//dim)*dim,stride):\n",
    "            for y in range(0,(img.shape[0]//dim)*dim,stride):\n",
    "                somme=0\n",
    "                for h in range(dim):\n",
    "                    for l in range(dim):\n",
    "                        somme+=img[x+l][y+h][rgb]\n",
    "                newImg[((x-dim)//stride)+1][((y-dim)//stride)+1][rgb]=somme//dim*dim\n",
    "    \n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bce79b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(nb_classes):\n",
    "    model=nn.Sequential(\n",
    "        nn.Conv2d(1,16,kernel_size=3,padding=1), #(1,100,100) -> (16,100,100)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2), #(16,100,100) -> ((100-2)//2)+1 = 49+1 = 50 donc -> (16,50,50)\n",
    "        nn.Conv2d(16,32,kernel_size=3,padding=1), #(16,50,50) -> (32,50,50)\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2), #(32,50,50) -> ((50-2)//2)+1 = 25 -> (32,25,25)\n",
    "        nn.Flatten(), #(32,25,25) -> (32*25*25) = (20000)\n",
    "        #nn.Dropout(0.3) #pour éviter l'overfitting\n",
    "        nn.Linear(20000,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,nb_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "502884af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_train(X_train,y_train,nb_epoch,labels,batch_size,model=None):\n",
    "    if model is None: #créé un model si on en donne pas\n",
    "        model=make_model(len(list(set(y_train))))\n",
    "    \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #set le device to the GPU or CPU\n",
    "    model.to(device) #envoie le model au device\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train) #prend 20% du dataset en validation\n",
    "    y_train_tensor=torch.tensor([labels[y] for y in y_train])\n",
    "\n",
    "    #X_train_tensor=[imgProcess(img,withTorch=True) for img in X_train] #applique l'imgprocess en mode torch aux images train\n",
    "    #X_train_tensor=torch.cat(X_train_tensor,dim=0) #stock toutes les images stockées dans X_train_tensor dans un conteneur tensor = créé le batch de taille len(X_train)\n",
    "    \n",
    "    #dataset=TensorDataset(X_train_tensor,y_train_tensor) #assemble nos data pour créer le dataset tensor\n",
    "    dataset=list(zip(X_train,y_train_tensor))\n",
    "    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=lambda x: x) #shuffle -> pour changer l'ordre des pairs img-label pour pas qu'on choppe des batchs uniquement du même label\n",
    "    #le dataloader va séparer le dataset de taille N en (N//batch_size)+1 si N%batch_size != 0 -> il y aura N//batch_size batchs complets et un batch plus petit avec les data restantes\n",
    "    #on peut rajouter le param drop_last=True si on veut supprimer le dernier batch -> +stabilité et certaines couches demandent un full batch (BatchNorm par ex)\n",
    "    val_dataset = list(zip(X_val, [labels[y] for y in y_val])) #validation dataset\n",
    "    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "    #+ le batch_size est élevé + ce sera précis mais + ça consommera\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss() #fonction de coût/perte à minimiser -> crossentropy utilisée pour la classification multiclasse\n",
    "    #applique softmax à la prédiction (qui sont des scores) pour transformer les scores en probabilités puis compare les proba à la classe cible et renvoie une erreur.\n",
    "    #plus la probabilité de la bonne classe est faible, plus la perte sera grande\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #lr = learning rate -> plus c'est petit plus c'est progressif et donc plus il aura besoin d'apprentissage mais plus il sera précis\n",
    "    #Adam = Adaptive Moment Estimation -> optimiseur pour réduire la perte. Adam adapte le lr localement donc pas besoin de régler le lr très finement. Utilise le SGD (Stochastic Gradient Descent)\n",
    "    #SGD -> minimisation de la perte pour chaque exemple individuellement là ou le classique GD minimise la perte globale -> SGD plus bruité (moins stable) mais + rapide\n",
    "\n",
    "    best_val_acc = 0\n",
    "    patience = 12\n",
    "    wait = 0\n",
    "    model.train() #active le mode training\n",
    "    for epoch in range(nb_epoch): # 1 epoque = le moment où le modèle a vu tout le dataset donc une époque = tous les (N//batch_size)+1 batchs\n",
    "        # plus il y a d'epoch + le model va apprendre et réduire l'erreur mais attention à l'overfitting\n",
    "        #-> petit dataset(>1000) = 10 to 50 epochs -> moyen dataset (1000 to 100k) = 20 to 100 et grand dataset (>100k) -> 10 to 50\n",
    "        #SINON -> utiliser early stopping pour être précis qui stop quand la précision sur validation baisse ou n'augmente plus -> à coder soit même\n",
    "        total_loss=0.0\n",
    "        correct=0\n",
    "        total=0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            X_raw,labels_batch=zip(*batch)\n",
    "            img=[imgProcess(image,withTorch=True) for image in X_raw]\n",
    "            img=torch.cat(img,dim=0)\n",
    "            img=img.to(device) #envoie les img au device\n",
    "            labels=torch.tensor(labels_batch).to(device) #envoie les labels au device, tout à besoin d'être sur le même device pour être traité\n",
    "\n",
    "            outputs=model(img) #prédiction des img (forward)\n",
    "            loss=criterion(outputs,labels) #calcul de la perte avec le criterion (ici crossentropy)\n",
    "\n",
    "            #rétropropagation (backward)\n",
    "            optimizer.zero_grad() #remet les gradients à 0\n",
    "            loss.backward()  #calcul les nouveaux gradients\n",
    "\n",
    "            optimizer.step() #mise à jour des poids\n",
    "\n",
    "            #partie statistiques (facultatif mais utile)\n",
    "            total_loss+=loss.item() #additionne la loss de chaque epoch\n",
    "            _,predicted=torch.max(outputs,1) #prend le score max de la dim 1 de output qui correspond à la classe et prend uniquement la classe associée à ce score max = les classes prédites\n",
    "            correct+=(predicted==labels).sum().item() #compte combien de ces preds sont correctes\n",
    "            total+=labels.size(0) #compte le nb d'img traitées dans cette epoch (labels.size(0)=32 ici vu que c'est le batch_size)\n",
    "            \n",
    "        acc=100*correct/total #bonnes prédictions sur l'ensemble de l'epoch en %\n",
    "        val_acc=CNN_evaluate(model=model,loader=val_loader,device=device) #calcule l'accuracy du dataset validation\n",
    "        print(f\"Epoch : {epoch+1}/{nb_epoch}\\n Perte :{total_loss:.4f}\\n Précision : {acc:.2f}%\\n Val accuracy : {val_acc*100:.2f}%\")\n",
    "        \n",
    "        if val_acc>best_val_acc:\n",
    "            best_val_acc=val_acc #si l'acc de la val est meilleure on l'assigne et on reset le timer \n",
    "            wait=0\n",
    "            best_model=copy.deepcopy(model.state_dict()) #on copie le meilleur model\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait>=patience: #on attend le nombre d'epoch max sans amélioration\n",
    "                print(\"Early stopping activated\")\n",
    "                break\n",
    "    \n",
    "    print(\"Training terminé.\")\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e7c90f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_evaluate(model,X_test=None,y_test=None,labels=None,batch_size=None,loader=None,device=None):\n",
    "    if device is None:\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "        \n",
    "    model.eval() #active le mode evaluation\n",
    "\n",
    "    if loader is None:\n",
    "        y_test_tensor=torch.tensor([labels[y] for y in y_test])\n",
    "        test_dataset=list(zip(X_test,y_test_tensor))\n",
    "        loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=True,collate_fn=lambda x: x)\n",
    "\n",
    "    correct=0\n",
    "    total=0\n",
    "\n",
    "    with torch.no_grad(): #pas de backward (juste forward = pred)\n",
    "        for batch in loader:\n",
    "            X_raw,labels_batch=zip(*batch)\n",
    "            img=[imgProcess(image,withTorch=True) for image in X_raw]\n",
    "            img=torch.cat(img,dim=0)\n",
    "            img=img.to(device)\n",
    "            labels=torch.tensor(labels_batch).to(device)\n",
    "\n",
    "            outputs=model(img)\n",
    "            _,pred=torch.max(outputs,1)\n",
    "\n",
    "            correct+=(pred==labels).sum().item()\n",
    "            total+=labels.size(0)\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e31e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_inf(img_path,model,labels):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img=imgProcess(img_path,withTorch=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output=model(img)\n",
    "        _,pred=torch.max(output,1)\n",
    "    \n",
    "    return labels[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "405fe295",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToPILImage(), #transforme en format PIL\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.RandomHorizontalFlip(), #0.5 de proba d'inverser la gauche et la droite de l'img pour rendre le modèle invariant à la symétrie\n",
    "    transforms.RandomRotation(10), #applique rota random entre -10° et +10° \n",
    "    transforms.ColorJitter(brightness=0.2,contrast=0.2), #altère aléatoirement la luminosité et le contraste de l'image\n",
    "    transforms.ToTensor() #reconvertit l'img en format tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaf8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n",
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sample : 125\n",
      "\n",
      "Number of training sample : 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#X_train,y_train,X_test,y_test=extract_data(\"Faces\",\"yes\")\n",
    "X_train,y_train=extract_data(\"train\")\n",
    "X_test,y_test=extract_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c31fedb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/25\n",
      " Perte :35.1194\n",
      " Précision : 18.00%\n",
      " Val accuracy : 20.00%\n",
      "Epoch : 2/25\n",
      " Perte :31.9392\n",
      " Précision : 22.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 3/25\n",
      " Perte :31.3482\n",
      " Précision : 30.00%\n",
      " Val accuracy : 20.00%\n",
      "Epoch : 4/25\n",
      " Perte :30.7051\n",
      " Précision : 30.00%\n",
      " Val accuracy : 16.00%\n",
      "Epoch : 5/25\n",
      " Perte :29.4934\n",
      " Précision : 32.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 6/25\n",
      " Perte :28.5953\n",
      " Précision : 36.00%\n",
      " Val accuracy : 20.00%\n",
      "Epoch : 7/25\n",
      " Perte :26.1142\n",
      " Précision : 47.00%\n",
      " Val accuracy : 36.00%\n",
      "Epoch : 8/25\n",
      " Perte :26.4392\n",
      " Précision : 47.00%\n",
      " Val accuracy : 16.00%\n",
      "Epoch : 9/25\n",
      " Perte :25.1557\n",
      " Précision : 49.00%\n",
      " Val accuracy : 16.00%\n",
      "Epoch : 10/25\n",
      " Perte :20.6250\n",
      " Précision : 61.00%\n",
      " Val accuracy : 28.00%\n",
      "Epoch : 11/25\n",
      " Perte :19.1364\n",
      " Précision : 68.00%\n",
      " Val accuracy : 28.00%\n",
      "Epoch : 12/25\n",
      " Perte :19.3987\n",
      " Précision : 57.00%\n",
      " Val accuracy : 16.00%\n",
      "Epoch : 13/25\n",
      " Perte :15.0952\n",
      " Précision : 76.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 14/25\n",
      " Perte :15.8600\n",
      " Précision : 71.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 15/25\n",
      " Perte :14.2160\n",
      " Précision : 75.00%\n",
      " Val accuracy : 20.00%\n",
      "Epoch : 16/25\n",
      " Perte :10.7348\n",
      " Précision : 83.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 17/25\n",
      " Perte :9.0357\n",
      " Précision : 85.00%\n",
      " Val accuracy : 24.00%\n",
      "Epoch : 18/25\n",
      " Perte :8.0536\n",
      " Précision : 89.00%\n",
      " Val accuracy : 20.00%\n",
      "Epoch : 19/25\n",
      " Perte :5.5730\n",
      " Précision : 91.00%\n",
      " Val accuracy : 28.00%\n",
      "Early stopping activated\n",
      "Training terminé.\n"
     ]
    }
   ],
   "source": [
    "labels={label:i for i,label in enumerate(sorted(set(y_train)))} #list(set(y_train)) sort les val uniques en list et sorted les range dans l'ordre alphabétique \n",
    "model=CNN_train(X_train=X_train,y_train=y_train,nb_epoch=25,labels=labels,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "16a314bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"{CNN_evaluate(X_test=X_test,y_test=y_test,model=model,labels=labels,batch_size=6)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2822a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convertion de l'img\n",
      "Augustin\n"
     ]
    }
   ],
   "source": [
    "id2label = {v: k for k, v in labels.items()}\n",
    "print(CNN_inf(\"test.jpg\",model,id2label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
